---
aliases:
  - Continuous Bag of Words
  - CBOW
---
## Overview
**Continuous Bag of Words (CBOW)** is a method used in [[Word2Vec]] for learning word embeddings. It predicts a target word based on the surrounding context words within a given window size. CBOW focuses on capturing the semantics of words by leveraging their contextual relationships in a corpus.

CBOW is one of two architectures introduced by [[Tomas Mikolov]] in the Word2Vec model, the other being [[Skip-Gram]]. It is computationally efficient and performs well for tasks where context prediction is critical.

---

## Key Concepts

### **1. Objective**
The primary goal of CBOW is to predict a target word $w_t$ given its context words $w_{t-k}, \dots, w_{t-1}, w_{t+1}, \dots, w_{t+k}$ within a window of size $k$. The model optimizes the following probability:
$$
P(w_t \mid w_{t-k}, \dots, w_{t-1}, w_{t+1}, \dots, w_{t+k})
$$
Where:
- $w_t$: Target word.
- $w_{t-k}, \dots, w_{t+k}$: Context words.

---

### **2. Architecture**
1. **Input Layer**:
   - Context words are represented as one-hot vectors.
2. **[[Hidden Layer]]**:
   - Compresses the input using a shared embedding matrix, creating a single vector representation.
3. **Output Layer**:
   - Produces a probability distribution over the vocabulary to predict the target word.
4. **Loss Function**:
   - Negative log likelihood is used to maximize the probability of the correct target word.

---

### **3. Comparison with Skip-Gram**
- **CBOW**:
  - Predicts the target word using its context.
  - Focuses on frequent words and works well with smaller datasets.
- **Skip-Gram**:
  - Predicts context words using the target word.
  - Performs better for rare words and larger datasets.

---

## Applications

1. **Word Embedding Generation**:
   - Creates dense vector representations of words that capture their semantic meanings.
2. **Semantic Similarity**:
   - Supports tasks like [[Semantic Similarity]], [[Text Classification]], and [[Named Entity Recognition (NER)]].
3. **Preprocessing for NLP Models**:
   - CBOW embeddings can be used as input features for downstream tasks.

---

## Advantages

1. **Efficiency**:
   - CBOW is computationally efficient due to its simple architecture.
2. **Contextual Understanding**:
   - Captures semantic relationships between words based on their context.
3. **Generalization**:
   - Learns embeddings that generalize well across tasks.

---

## Disadvantages

1. **Focus on Frequent Words**:
   - CBOW tends to favor frequent words, which can lead to poor embeddings for rare words.
2. **Limited Context**:
   - The fixed window size may not capture long-range dependencies.
3. **Simplicity**:
   - Lacks the complexity to model nuanced language phenomena compared to modern models like [[Transformers]].

---

## Example Workflow

1. **Prepare Data**:
   - Tokenize text and create a sliding window of context-target pairs.
2. **Train CBOW Model**:
   - Use frameworks like Gensim or TensorFlow to train the model.
3. **Generate Embeddings**:
   - Extract the learned word embeddings for use in downstream tasks.

---

## Tools and Libraries

1. **Gensim**:
   - Provides prebuilt implementations for CBOW and [[Skip-Gram]].
2. **TensorFlow**:
   - Allows for custom implementation of CBOW.
3. **PyTorch**:
   - Useful for building CBOW models with flexibility.

---

## Related Topics

- [[Word2Vec]]: The overarching framework that introduced CBOW and Skip-Gram.
- [[Skip-Gram]]: A complementary method to CBOW in Word2Vec.
- [[Embeddings]]: Dense vector representations learned using CBOW.
- [[Text Classification]]: A common downstream task for embeddings generated by CBOW.
- [[Semantic Similarity]]: CBOW embeddings are often used for similarity computations.

---

## Aliases
- CBOW
- Continuous Bag of Words

---

## Tags
#CBOW #Word2Vec #embeddings #NLP #machinelearning #textprocessing

---

## Links to Explore
- [[Word2Vec]]: Understand the larger framework encompassing CBOW.
- [[Skip-Gram]]: Compare CBOW with its complementary architecture.
- [[Embeddings]]: Learn how CBOW generates dense vector representations.
- [[Semantic Similarity]]: Explore applications of CBOW in measuring text similarity.
- [[Gensim]]: Discover tools for implementing CBOW models.